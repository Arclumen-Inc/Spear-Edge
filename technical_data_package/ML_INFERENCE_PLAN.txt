================================================================================
ML INFERENCE IMPLEMENTATION PLAN
SPEAR-Edge RF Signal Classification
================================================================================
Document: ML_INFERENCE_PLAN.txt
Purpose: Comprehensive plan for implementing ML-based RF signal classification
         on Jetson Orin using captured spectrogram data
Date: 2025-01-05

================================================================================
1. CURRENT STATE
================================================================================

1.1 Infrastructure
-------------------
✓ ONNX Runtime installed (CPU provider, works on Jetson)
✓ ONNX classifier implementation (infer_onnx.py)
✓ Capture pipeline generates ML-ready spectrograms (512x512 float32)
✓ Dataset export system (data/dataset_raw/)
✓ Classification integration in capture_manager.py
✓ Fallback chain: ONNX → Orchestrator → Stub

1.2 Data Format
---------------
- Input: 512×512 float32 spectrogram (noise-floor normalized)
- Format: NumPy array (.npy files)
- Location: data/artifacts/captures/{timestamp}_*/features/spectrogram.npy
- Metadata: capture.json with RF config, quality metrics, triage

1.3 Current Limitations
------------------------
- CPU-only inference (GPU providers not available with current onnxruntime)
- No trained model (using dummy model)
- Classification only runs if signal present and not noise
- Dataset export only happens when classification succeeds

================================================================================
2. DATA COLLECTION & PREPARATION
================================================================================

2.1 Capture Data Structure
---------------------------
Each capture contains:
- spectrogram.npy: (512, 512) float32, noise-floor normalized
- capture.json: Metadata (freq, sample_rate, quality, triage, etc.)
- thumbnails/: PNG spectrogram thumbnails
- samples.iq: Raw IQ data (for future use)

2.2 Dataset Organization
-------------------------
Current export location: data/dataset_raw/{capture_dir_name}/
- spectrogram.npy
- capture.json
- thumbnails/

Recommended structure for training:
data/dataset/
├── train/
│   ├── class_0/
│   │   ├── sample_001/
│   │   │   ├── spectrogram.npy
│   │   │   └── metadata.json
│   │   └── ...
│   ├── class_1/
│   └── ...
├── val/
│   └── (same structure)
└── test/
    └── (same structure)

2.3 Data Preparation Script
----------------------------
Create: scripts/prepare_training_dataset.py

Purpose:
- Scan data/dataset_raw/ for all captures
- Extract spectrograms and metadata
- Organize by class labels (from classification or manual labeling)
- Split into train/val/test (80/10/10)
- Generate dataset manifest (JSON with paths and labels)
- Validate data quality (check for corrupted files, wrong shapes)

Features:
- Support manual labeling via CSV/JSON mapping
- Support automatic labeling from existing classifications
- Data augmentation (optional): time/freq shifts, noise injection
- Balance classes (oversample minority classes if needed)

================================================================================
3. MODEL DEVELOPMENT (ON DEVELOPMENT MACHINE)
================================================================================

3.1 Model Architecture Options
-------------------------------
Option A: CNN (Recommended for spectrograms)
- Input: (1, 512, 512) - single channel spectrogram
- Architecture: ResNet-like or custom CNN
- Output: (num_classes,) - class probabilities
- Advantages: Good for spatial patterns, proven for spectrograms

Option B: Vision Transformer (ViT)
- Input: (1, 512, 512) - patch into tokens
- Architecture: Transformer encoder
- Output: (num_classes,)
- Advantages: Attention mechanism, good for complex patterns

Option C: Hybrid CNN + RNN
- CNN for spatial features, RNN for temporal patterns
- More complex, may be overkill for static spectrograms

Recommended: Start with Option A (CNN), can upgrade later

3.2 Model Training Framework
-----------------------------
Environment: Development machine (not Jetson)
Framework: PyTorch (easiest to convert to ONNX)

Training script: scripts/train_rf_classifier.py

Steps:
1. Load dataset (from prepare_training_dataset.py output)
2. Create DataLoader with augmentation
3. Define model architecture
4. Training loop:
   - Loss: CrossEntropyLoss
   - Optimizer: AdamW with learning rate scheduling
   - Metrics: Accuracy, F1-score, confusion matrix
5. Validation on holdout set
6. Save best model checkpoint
7. Export to ONNX format

3.3 Model Architecture Example (PyTorch)
------------------------------------------
```python
class RFClassifier(nn.Module):
    def __init__(self, num_classes=5):
        super().__init__()
        # Feature extraction
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.5)
        
        # Classification head
        # After 3 pooling ops: 512 -> 256 -> 128 -> 64
        self.fc1 = nn.Linear(128 * 64 * 64, 512)
        self.fc2 = nn.Linear(512, num_classes)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.pool(x)
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = self.relu(self.conv3(x))
        x = self.pool(x)
        x = x.view(-1, 128 * 64 * 64)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x
```

3.4 Training Hyperparameters
-----------------------------
- Batch size: 32 (adjust based on GPU memory)
- Learning rate: 1e-3 with cosine annealing
- Epochs: 50-100 (early stopping)
- Weight decay: 1e-4
- Data augmentation: Random horizontal flip, small rotations

3.5 Model Export to ONNX
--------------------------
After training, export to ONNX:
```python
model.eval()
dummy_input = torch.randn(1, 1, 512, 512)
torch.onnx.export(
    model,
    dummy_input,
    "rf_classifier.onnx",
    input_names=["spectrogram"],
    output_names=["logits"],
    dynamic_axes={"spectrogram": {0: "batch"}, "logits": {0: "batch"}},
    opset_version=11
)
```

================================================================================
4. MODEL DEPLOYMENT (ON JETSON ORIN)
================================================================================

4.1 Transfer Model to Jetson
------------------------------
1. Copy ONNX model file to: spear_edge/ml/models/rf_classifier.onnx
2. Update capture_manager.py to use new model path
3. Verify model loads correctly

4.2 Model Optimization (Optional)
----------------------------------
For better performance on Jetson:
- Quantize to INT8 (if accuracy acceptable)
- Use TensorRT provider (if available)
- Optimize ONNX graph (onnxoptimizer)

4.3 Integration Testing
------------------------
1. Load model in capture_manager
2. Test with real captures
3. Verify classification results
4. Monitor inference time (target: <50ms per spectrogram)
5. Check memory usage

================================================================================
5. CLASSIFICATION WORKFLOW
================================================================================

5.1 Current Flow
-----------------
1. Capture IQ data → disk
2. Compute spectrogram (512×512) → spec_ml
3. Compute triage (signal_present, likely_noise, etc.)
4. If signal_present and not likely_noise:
   - Run classifier.classify(spec_ml)
   - Store classification in capture.json
   - Export to dataset_raw/ if classification available
5. Continue with artifact writing

5.2 Classification Output Format
----------------------------------
{
    "label": "class_0",           # Predicted class
    "confidence": 0.95,            # Confidence [0.0, 1.0]
    "topk": [                     # Top-5 predictions
        {"label": "class_0", "p": 0.95},
        {"label": "class_1", "p": 0.03},
        ...
    ],
    "model": "onnx"               # Model identifier
}

5.3 Classification Labels
--------------------------
Define class mapping:
- class_0: "noise" or "unknown"
- class_1: "fhss_control" (e.g., ELRS, Crossfire)
- class_2: "digital_video" (e.g., DJI, Walksnail)
- class_3: "analog_video" (FPV analog)
- class_4: "voice_ptt" (narrowband voice)
- class_5: (add more as needed)

Store mapping in: spear_edge/ml/models/class_labels.json

================================================================================
6. PERFORMANCE OPTIMIZATION
================================================================================

6.1 Inference Speed Targets
----------------------------
- CPU inference: <100ms per spectrogram
- GPU inference (if available): <20ms per spectrogram
- Total capture time impact: <5% overhead

6.2 Optimization Strategies
----------------------------
1. Batch processing (if multiple captures queued)
2. Async inference (don't block capture pipeline)
3. Model quantization (INT8 if accuracy acceptable)
4. Use TensorRT provider (if available on Jetson)
5. Cache model in memory (already done)

6.3 Memory Management
---------------------
- Model size: Keep <100MB
- Inference memory: <200MB peak
- Total overhead: <300MB for ML

================================================================================
7. TESTING & VALIDATION
================================================================================

7.1 Unit Tests
---------------
Create: tests/test_onnx_classifier.py
- Test model loading
- Test inference with dummy data
- Test error handling (missing model, wrong shape, etc.)
- Test output format

7.2 Integration Tests
----------------------
Create: tests/test_capture_with_classification.py
- Test full capture pipeline with classification
- Verify classification appears in capture.json
- Verify dataset export works
- Test fallback to stub if model unavailable

7.3 Validation Dataset
----------------------
- Reserve 10% of captures for validation
- Test model accuracy on real captures
- Compare with ground truth (manual labels)
- Track confusion matrix over time

================================================================================
8. MONITORING & LOGGING
================================================================================

8.1 Classification Metrics
---------------------------
Log to capture.json:
- Inference time (ms)
- Model version
- Input shape validation
- Confidence distribution

8.2 Performance Monitoring
---------------------------
Track:
- Average inference time
- Classification success rate
- Model accuracy (if ground truth available)
- Memory usage

8.3 Error Handling
------------------
- Graceful degradation if model fails
- Log classification errors
- Fallback to stub classifier
- Don't block captures if ML fails

================================================================================
9. FUTURE IMPROVEMENTS
================================================================================

9.1 Model Improvements
----------------------
- Active learning: Use low-confidence predictions for retraining
- Online learning: Update model with new labeled data
- Ensemble models: Combine multiple models
- Multi-task learning: Classify + detect + localize

9.2 Infrastructure Improvements
--------------------------------
- GPU acceleration (Jetson-specific onnxruntime-gpu)
- TensorRT optimization (convert ONNX → TensorRT engine)
- Distributed inference (if multiple Jetsons)
- Model versioning and A/B testing

9.3 Data Pipeline Improvements
-------------------------------
- Automatic labeling from triage metadata
- Data quality filtering (remove bad captures)
- Balanced dataset generation
- Synthetic data generation

================================================================================
10. IMPLEMENTATION CHECKLIST
================================================================================

Phase 1: Data Collection (Current)
[✓] Capture pipeline generates ML-ready spectrograms
[✓] Dataset export system working
[ ] Collect sufficient training data (target: 1000+ samples per class)
[ ] Manual labeling of collected data
[ ] Create training dataset structure

Phase 2: Model Development (Development Machine)
[ ] Set up PyTorch training environment
[ ] Create data preparation script
[ ] Design model architecture
[ ] Implement training script
[ ] Train initial model
[ ] Validate on holdout set
[ ] Export to ONNX format
[ ] Test ONNX model locally

Phase 3: Deployment (Jetson)
[ ] Transfer ONNX model to Jetson
[ ] Update model path in capture_manager
[ ] Test model loading
[ ] Test inference with real captures
[ ] Verify classification in capture.json
[ ] Monitor performance metrics
[ ] Optimize if needed

Phase 4: Production
[ ] Deploy trained model
[ ] Monitor classification accuracy
[ ] Collect feedback for model improvement
[ ] Plan model updates/retraining

================================================================================
11. FILE STRUCTURE
================================================================================

spear_edge/
├── ml/
│   ├── infer_onnx.py          ✓ Created
│   ├── infer_stub.py          ✓ Exists
│   ├── models/
│   │   ├── rf_classifier.onnx  (to be created)
│   │   ├── class_labels.json   (to be created)
│   │   └── spear_dummy.onnx    ✓ Exists (placeholder)
│   └── __init__.py             ✓ Updated
│
├── core/capture/
│   └── capture_manager.py     ✓ Updated with ONNX support
│
└── scripts/                    (to be created)
    ├── prepare_training_dataset.py
    ├── train_rf_classifier.py
    └── export_to_onnx.py

data/
├── dataset_raw/                ✓ Auto-export working
└── dataset/                     (for training)
    ├── train/
    ├── val/
    └── test/

================================================================================
12. QUICK START GUIDE
================================================================================

12.1 Collect Training Data
---------------------------
1. Run captures in armed mode
2. Let system collect captures with classifications
3. Manually label captures in data/dataset_raw/
4. Run prepare_training_dataset.py to organize data

12.2 Train Model (Development Machine)
---------------------------------------
1. Install PyTorch: pip install torch torchvision
2. Run: python scripts/prepare_training_dataset.py
3. Run: python scripts/train_rf_classifier.py
4. Export: python scripts/export_to_onnx.py
5. Copy model to Jetson: spear_edge/ml/models/rf_classifier.onnx

12.3 Deploy on Jetson
---------------------
1. Update capture_manager.py model path (if needed)
2. Restart application
3. Verify model loads: Check logs for "[ONNX] Model loaded"
4. Test with capture: Verify classification in capture.json

================================================================================
13. TROUBLESHOOTING
================================================================================

13.1 Model Won't Load
---------------------
- Check model file exists and is readable
- Verify ONNX model is valid: onnx.checker.check_model()
- Check input/output shapes match expectations
- Review error logs for specific issues

13.2 Inference Fails
--------------------
- Verify input shape is (512, 512)
- Check input dtype is float32
- Ensure model expects (1, 1, 512, 512) input
- Test with dummy data first

13.3 Poor Performance
---------------------
- Check inference time in logs
- Consider model quantization
- Try TensorRT provider (if available)
- Optimize ONNX graph
- Reduce model complexity if needed

13.4 Low Accuracy
----------------
- Collect more training data
- Improve data quality (remove noise-only captures)
- Adjust model architecture
- Try data augmentation
- Check class balance in training set

================================================================================
14. RESOURCES & REFERENCES
================================================================================

14.1 Documentation
------------------
- ONNX Runtime: https://onnxruntime.ai/docs/
- PyTorch: https://pytorch.org/docs/
- Jetson Inference: https://github.com/dusty-nv/jetson-inference

14.2 Tools
----------
- ONNX Model Zoo: Pre-trained models
- Netron: Visualize ONNX models
- ONNX Optimizer: Optimize ONNX graphs

14.3 Best Practices
-------------------
- Start simple, iterate
- Collect diverse training data
- Validate on real-world data
- Monitor performance in production
- Plan for model updates

================================================================================
END OF PLAN
================================================================================



